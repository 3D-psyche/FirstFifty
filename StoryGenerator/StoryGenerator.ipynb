{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import quote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tag import StanfordPOSTagger, StanfordNERTagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stanford POS tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setting the path to java.exe for nltk\n",
    "java_path = \"F:/java/java/bin/java.exe\"\n",
    "    \n",
    "os.environ['JAVAHOME'] = java_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "## creating a StanfordPOStagger object\n",
    "pos_base_path = \"E:\\Stanford Taggers\\stanford-postagger-full-2015-04-20\\stanford-postagger-full-2015-04-20\"\n",
    "\n",
    "st_pos = StanfordPOSTagger(model_filename = pos_base_path+\"\\models\\english-bidirectional-distsim.tagger\", path_to_jar = pos_base_path+\"\\stanford-postagger.jar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "## creating a StanfordPOStagger object\n",
    "ner_base_path = \"E:\\Stanford Taggers\\stanford-ner-4.2.0\\stanford-ner-2020-11-17\"\n",
    "    \n",
    "st_ner = StanfordNERTagger(model_filename = ner_base_path+\"\\classifiers\\english.all.3class.distsim.crf.ser.gz\", path_to_jar = ner_base_path+\"\\stanford-ner-4.2.0.jar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "# st_ner.tag(word_tokenize(\"Hyderabad is my favorate place\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "# st_pos.tag(word_tokenize(\"Hyderabad is my favorate place\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to return a random plot form data frame with Title and Genre\n",
    "def sel_plot(df):\n",
    "    # choosing a random value \n",
    "    num = random.choice([100, 150, 200, 250])\n",
    "    val = random.randrange(1,num)\n",
    "\n",
    "    print(f\"Selected value : {val}\")\n",
    "    \n",
    "    ## re-framing the data\n",
    "    plots = {}\n",
    "    for index,row in df.iterrows():\n",
    "        plots[row[\"Title\"]] = {\"Genre\":row[\"Genre\"], \"Plot\": row[\"Plot\"]}\n",
    "        if index == val: break\n",
    "            \n",
    "    # choosing a random title\n",
    "    titles = list(plots.keys())\n",
    "    sel_movie = random.choice(titles)\n",
    "    \n",
    "    return plots[sel_movie]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "## function to tokenize the plot into Parts of Speech\n",
    "# func to get abb of pos\n",
    "def get_abb(tag_obj):\n",
    "    pos_dct = {}\n",
    "    ner_dct = {}\n",
    "\n",
    "    if type(tag_obj) == str: \n",
    "        pos_tags = st_pos.tag(tag_obj.split())\n",
    "        ner_tags = st_ner.tag(tag_obj.split())\n",
    "    elif type(tag_obj) == list: \n",
    "        pos_tags = st_pos.tag(tag_obj)\n",
    "        ner_tags = st_ner.tag(tag_obj)\n",
    "        \n",
    "    for word,tag in pos_tags:\n",
    "        try:\n",
    "            pos_dct[word] = tag, abb[tag] \n",
    "        except:\n",
    "            pos_dct[word] = tag \n",
    "    for word,tag in ner_tags:\n",
    "        ner_dct[word] = tag\n",
    "\n",
    "        \n",
    "    return {\"Parts of Speech\": pos_dct, \"Named Entities\": ner_dct}\n",
    "\n",
    "# func to get tokens\n",
    "def tokenizer(plot_obj):\n",
    "    plot = plot_obj[\"Plot\"]\n",
    "    get_dct = get_abb(plot)\n",
    "      \n",
    "    return plot_obj[\"Genre\"],get_dct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer(sel_plot(req_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetching Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fetching the movie data\n",
    "movies_data = pd.read_csv(\"wiki_movie_plots_deduped.csv\", encoding=\"utf-8\")\n",
    "\n",
    "req_data = movies_data[[\"Title\", \"Genre\", \"Plot\"]]\n",
    "req_data = req_data[req_data[\"Genre\"] != \"unknown\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cleaning the data\n",
    "# cleaning the plot encoding\n",
    "def clean_str(st):\n",
    "    for ch in st :\n",
    "        if ch in '!#$%&\\()*+-/:;<=>@[\\\\]^_`{|}~123456789':\n",
    "            st = st.replace(ch,\"\")\n",
    "    st = st.encode(\"ascii\", \"ignore\").decode(\"utf-8\")\n",
    "    return st\n",
    "\n",
    "req_data[\"Plot\"] = req_data[\"Plot\"].apply(lambda x:clean_str(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving the required data\n",
    "req_data.to_csv(\"data_subset.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS tags Abrrevations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\"\n",
    "res = requests.get(url)\n",
    "soup = bs(res.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = soup.select(\"table tr\")\n",
    "\n",
    "abb = {}\n",
    "for row in table[1:]:\n",
    "    cols = row.find_all(\"td\")\n",
    "    col_lst = [col.get_text() for col in cols][1:]\n",
    "    abb[col_lst[0].strip()] = col_lst[1].strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nltk Stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### creating a word list of stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_wrds = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_abb = get_abb(eng_wrds[:150])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_dct = {}\n",
    "\n",
    "for key,value in eng_abb.items():\n",
    "    if value[1] not in eng_dct:\n",
    "        eng_dct[value[1]] = list()\n",
    "    eng_dct[value[1]].append(key)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eng_dct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## StoryGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected value : 65\n"
     ]
    }
   ],
   "source": [
    "res = pos_tokenizer(sel_plot(req_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [],
   "source": [
    "genre = res[0]\n",
    "pos_res = res[1][\"Parts of Speech\"]\n",
    "ner_res = res[1][\"Named Entities\"]\n",
    "words = list(pos_res.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = random.randrange(5, len(words)//5)\n",
    "pos_choosen = []\n",
    "for i in range(num):\n",
    "    pos_choosen.append(random.choice(words))   \n",
    "    \n",
    "ner_choosen = [wrd for wrd,ner in list(ner_res.items()) if ner != \"O\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['husband.', 'gets', 'rivals,', 'as', 'ball', 'ensues.', 'and']"
      ]
     },
     "execution_count": 507,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_choosen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "More on LOCATION at: https://www.google.com/search?q=LOCATION\n",
      "Enter a LOCATION:\n",
      "\n",
      "More on LOCATION at: https://www.google.com/search?q=LOCATION\n",
      "Enter a LOCATION:\n",
      "\n",
      "More on Preposition or subordinating conjunction at: https://www.google.com/search?q=Preposition%20or%20subordinating%20conjunction\n",
      "Enter a Preposition or subordinating conjunction:\n",
      "\n",
      "More on Coordinating conjunction at: https://www.google.com/search?q=Coordinating%20conjunction\n",
      "Enter a Coordinating conjunction:\n",
      "\n",
      "More on Noun, singular or mass at: https://www.google.com/search?q=Noun%2C%20singular%20or%20mass\n",
      "Enter a Noun, singular or mass:\n",
      "\n",
      "More on Verb, 3rd person singular present at: https://www.google.com/search?q=Verb%2C%203rd%20person%20singular%20present\n",
      "Enter a Verb, 3rd person singular present:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Asking user for some words based on their\n",
    "new_words = []\n",
    "occurred = [] \n",
    "for wrd,tup in pos_res.items():    \n",
    "    if tup[1] not in occurred :\n",
    "        if (wrd not in ner_choosen) & (wrd in pos_choosen):  \n",
    "            occurred.append(tup[1])\n",
    "            print (f\"More on {tup[1]} at: https://www.google.com/search?q={quote(tup[1])}\")\n",
    "            new_word = input(f\"Enter a {tup[1]}:\")    \n",
    "            if len(new_word) < 1: new_words.append(wrd)\n",
    "            new_words.append(new_word)\n",
    "            print(\"\")\n",
    "            \n",
    "        elif wrd in ner_choosen:\n",
    "            print (f\"More on {ner_res[wrd]} at: https://www.google.com/search?q={quote(ner_res[wrd])}\")\n",
    "            new_word = input(f\"Enter a {ner_res[wrd]}:\")    \n",
    "            if len(new_word) < 1: new_words.append(wrd)\n",
    "            new_words.append(new_word)\n",
    "            print(\"\")\n",
    "    elif tup[1] in occurred:        \n",
    "        try:\n",
    "            new_word = random.choice(eng_dct[tup[1]])\n",
    "            new_words.append(new_word)\n",
    "        except:\n",
    "            new_words.append(wrd) \n",
    "    else:\n",
    "        new_words.append(wrd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"New  York  as  and  until above ball  don't s t or s o s don't don't gets  it's with t o is don't is o\""
      ]
     },
     "execution_count": 509,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(new_words)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
